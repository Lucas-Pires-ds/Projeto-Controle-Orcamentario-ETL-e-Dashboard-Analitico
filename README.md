# Projeto: Controle Or√ßament√°rio - De ponta a ponta (ETL, Data Quality e Analytics)

## üìå Vis√£o Geral
Este √© um projeto de **An√°lise de Dados** focado em controle or√ßament√°rio e lan√ßamentos financeiros. O diferencial deste projeto √© a implementa√ß√£o de um pipeline de **ETL com alicerces de Engenharia de Dados**, garantindo que as an√°lises finais no Power BI sejam baseadas em dados √≠ntegros, audit√°veis e livres de inconsist√™ncias.

---

## üèóÔ∏è Arquitetura e Estrutura do Pipeline
O projeto foi desenhado utilizando o conceito de camadas, garantindo a separa√ß√£o entre o dado bruto e o dado pronto para an√°lise:

1.  **Staging Layer (`stg_`)**: Camada de aterrissagem dos dados. Aqui, os dados s√£o importados "como est√£o", permitindo a identifica√ß√£o de ru√≠dos, duplicidades e erros de preenchimento gerados propositalmente por um script Python de simula√ß√£o.
2.  **Diagn√≥stico de Qualidade (Data Quality)**: Uma etapa intermedi√°ria (alicerce de engenharia) onde o dado √© auditado via SQL antes de qualquer transforma√ß√£o.
3.  **Trusted Layer (Dimens√µes e Fatos)**: Camada final de dados limpos, tipados e padronizados, servindo como a √∫nica "fonte da verdade" para o Dashboard.

---

## üõ†Ô∏è Tecnologias Utilizadas
* **SQL Server**: Motor principal para processamento, limpeza e modelagem.
* **Python**: Gera√ß√£o de dados sint√©ticos com regras de sazonalidade e erros controlados.
* **Power BI**: (Em constru√ß√£o) Camada de visualiza√ß√£o e c√°lculo de KPIs.

---

## üìà Log de Desenvolvimento (Metodologia)

### [28/12/2025] Ingest√£o e Estrutura Inicial
* Configura√ß√£o do ambiente e cria√ß√£o da estrutura de banco de dados.
* Carga de 5000+ registros via Bulk Insert na camada de Staging.
* **Decis√£o t√©cnica:** Uso de **Views** para isolar a l√≥gica de tratamento, permitindo testar a limpeza antes da carga f√≠sica.

### [03/01/2026] Refatora√ß√£o: Implementando a Camada de Data Quality
Neste est√°gio, o projeto foi elevado para um n√≠vel de **Analytics Engineering**. Em vez de apenas limpar os dados, criei um pipeline de valida√ß√£o:
* **Detec√ß√£o de "Sujeira" de String:** Implementa√ß√£o da l√≥gica `LEN(col) > LEN(TRIM(col))` para monitorar automaticamente espa√ßos extras.
* **Tratamento de Dados Vazios:** Valida√ß√£o composta `IS NULL OR LEN(col) = 0` para capturar aus√™ncia de dados que o banco n√£o reconhece como nulo.
* **Auditoria de Unicidade:** Uso de `GROUP BY` e `HAVING` para garantir a integridade das Chaves Prim√°rias (PKs) antes da carga na Trusted.
* **Padroniza√ß√£o Sem√¢ntica:** Uso de fun√ß√µes de string para garantir o formato *Initcap* (Primeira letra mai√∫scula) em todas as dimens√µes.

---

## üöÄ Pr√≥ximos Passos
- [ ] Aplicar a r√©gua de Data Quality nas Tabelas Fato (`fato_lancamentos` e `fato_orcamento`).
- [ ] Implementar valida√ß√£o de integridade referencial (Chaves Estrangeiras).
- [ ] Desenvolver o Dashboard no Power BI com foco em indicadores de desvio or√ßament√°rio e tend√™ncia.

---

**Autor:** Lucas Pires  
**Perfil:** Analista de Dados com foco em processos de ETL e Qualidade de Dados.